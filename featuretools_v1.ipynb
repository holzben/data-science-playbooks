{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with Featurtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example [featuretools](https://www.featuretools.com/) a python frame for automated feature engineering is used to generate a wide range of features for machine learning. The aim of the Notebook is to show how the framework is used and how it could support ML workflow, but should not be considered a tutorial on Machine Learning.\n",
    "\n",
    "One of the main benefits of the framework is the so-called deep feature synthesis (dfs), which used relations in the data for automatically generating features, this could be, obviously depending on your data, features like mean session length, average purchasing price, or standard deviation of time since the last session. The term deep comes refers to stacking generated features on top of each other. \n",
    "\n",
    "The main building block of Featuretools is feature primitives and can be represented in two main groups: aggregation and transformation primitive. As the name indicates, aggregation primitives take related inputs and return a single output, similar to group by aggregations known from sql. Transformation primitives, on the other hand, are converting, eg timestamps into data, hours, minutes, seconds, etc, or calculating differences in certain events. The full list of possible primitives can be found [here](https://primitives.featurelabs.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import featuretools as ft\n",
    "import xgboost as xgb\n",
    "\n",
    "from featuretools.selection import (\n",
    "    remove_highly_null_features,\n",
    "    remove_single_value_features,\n",
    ")\n",
    "\n",
    "from woodwork.logical_types import Categorical, Boolean, Datetime, Double\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from uuid import uuid4\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# from scipy.stats import uniform, randint\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# , FunctionTransformer\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from typing import List, Any\n",
    "# from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# pd.option_context(\"max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we are using the classic Telco Customer Churn dataset which could be found on [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn?select=WA_Fn-UseC_-Telco-Customer-Churn.csv), also the description and column definition could be found there, IBM provide a csv in their Githup repo which could be directly sourced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_TO_DATA = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
    "BACK_COUNT_DATE = datetime.fromisoformat(\"2022-01-01\")\n",
    "TEST_SIZE = 0.2\n",
    "VALID_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "NUMERIC_TRANSFORMER_REPLACEMENT = \"median\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_by_type(\n",
    "    in_df: pd.DataFrame, types_to_search: list, exclude_ids: Boolean = True\n",
    ") -> list:\n",
    "    \"\"\"Small helper function selecting columns according to their type.\"\"\"\n",
    "    cols = in_df.select_dtypes(include=types_to_search).columns.to_list()\n",
    "\n",
    "    if exclude_ids:\n",
    "        return list(filter(lambda x: not x.endswith(\"ID\"), cols))\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ft titanic https://www.kaggle.com/code/liananapalkova/automated-feature-engineering-for-titanic-dataset/notebook\n",
    "- the dataset https://github.com/IBM/telco-customer-churn-on-icp4d/blob/master/data/Telco-Customer-Churn.csv\n",
    "- titanic https://medium.com/dataexplorations/tool-review-can-featuretools-simplify-the-process-of-feature-engineering-5d165100b0c3\n",
    "- time indexing recomandations https://stackoverflow.com/questions/49711987/how-do-i-prevent-data-leakage-with-featuretools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of CERTIFICATE_VERIFY_FAILED run Install Certificates.command\n",
    "# see also https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org\n",
    "df = pd.read_csv(filepath_or_buffer=URL_TO_DATA, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since featuretools heavily rely on relations within the data, we split up the dataframe into three entities: Customers, Subscriptions and Billing, and introduce IDs to represent the relationship between them. \n",
    "\n",
    "- customer_df: customer_id (PK), subscription_id (FK), billing_id (FK), gender, SeniorCitizen, Partner, Dependents, tenure, Churn\n",
    "- services_df: service_id (PK), service_name\n",
    "- subscription_df: subscription_id (PK), service_id, customer_id\n",
    "- billing_df: billing_id (PK), Contract, PaperlessBilling, PaymentMethod, MonthlyCharges, TotalCharges\n",
    "\n",
    "PK indicating primary keys and FK foreign keys. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping between column names and featuretools data types.\n",
    "CUSTOMERS = {\n",
    "    \"gender\": Boolean,\n",
    "    \"SeniorCitizen\": Boolean,\n",
    "    \"Partner\": Boolean,\n",
    "    \"Dependents\": Boolean,\n",
    "}\n",
    "\n",
    "\n",
    "SUBSCRIPTIONS = {\n",
    "    \"PhoneService\": Categorical,\n",
    "    \"MultipleLines\": Boolean,\n",
    "    \"InternetService\": Categorical,\n",
    "    \"OnlineSecurity\": Categorical,\n",
    "    \"OnlineBackup\": Categorical,\n",
    "    \"DeviceProtection\": Categorical,\n",
    "    \"TechSupport\": Categorical,\n",
    "    \"StreamingTV\": Categorical,\n",
    "    \"StreamingMovies\": Categorical,\n",
    "}\n",
    "\n",
    "\n",
    "BILLINGS = {\n",
    "    \"tenure\": Double,\n",
    "    \"Contract\": Categorical,\n",
    "    \"PaperlessBilling\": Boolean,\n",
    "    \"PaymentMethod\": Categorical,\n",
    "    \"MonthlyCharges\": Double,\n",
    "    \"TotalCharges\": Double,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EntitySetColumns:\n",
    "    \"\"\"Small helper class for organizing dataframes and columns.\"\"\"\n",
    "\n",
    "    new_dataframe_name: str\n",
    "    index: str\n",
    "    additional_columns: list\n",
    "\n",
    "\n",
    "entity_set_columns = {\n",
    "    \"customers\": EntitySetColumns(\n",
    "        index=\"customerID\",\n",
    "        new_dataframe_name=None,\n",
    "        additional_columns=list(CUSTOMERS.keys()),\n",
    "    ),\n",
    "    \"subscriptions\": EntitySetColumns(\n",
    "        index=\"subscriptionID\",\n",
    "        new_dataframe_name=\"subscriptions\",\n",
    "        additional_columns=list(SUBSCRIPTIONS.keys()),\n",
    "    ),\n",
    "    \"billings\": EntitySetColumns(\n",
    "        index=\"billingID\",\n",
    "        new_dataframe_name=\"billings\",\n",
    "        additional_columns=list(BILLINGS.keys()),\n",
    "    ),\n",
    "}\n",
    "\n",
    "# adding PK's to the data\n",
    "df[\"customerID\"] = df.index\n",
    "df[\"billingID\"] = [str(uuid4()) for _ in range(df.shape[0])]\n",
    "df[\"subscriptionID\"] = [str(uuid4()) for _ in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further we need to clean the data a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert total charges to numeric\n",
    "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
    "\n",
    "# generate syntetic time index\n",
    "df[\"ContractStartDate\"] = list(\n",
    "    map(\n",
    "        lambda tenure, dat=BACK_COUNT_DATE: dat - relativedelta(months=-tenure),\n",
    "        df[\"tenure\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# convert to 1/0\n",
    "df[\"Churn\"] = np.where(df[\"Churn\"] == \"Yes\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not some features require domain knowledge, not all features could be generated automatically, for example, categorize the length of a contract or how many products the user subscribed to, which could not be determined automatically in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more features\n",
    "df[\"ShortContract\"] = df[\"Contract\"] == \"Month-to-month\"\n",
    "BILLINGS[\"ShortContract\"] = Boolean\n",
    "\n",
    "# remove because it depends on Phone service\n",
    "products = [n for n in list(SUBSCRIPTIONS.keys()) if n != \"MultipleLines\"]\n",
    "df[\"TotalProductCount\"] = (df[products] != \"No\").sum(axis=1)\n",
    "SUBSCRIPTIONS[\"TotalProductCount\"] = Double\n",
    "\n",
    "df[\"UserCategory\"] = pd.cut(\n",
    "    x=df[\"TotalProductCount\"], bins=[0, 3, 6, np.Inf], labels=[\"light\", \"mid\", \"heavy\"]\n",
    ")\n",
    "CUSTOMERS[\"UserCategory\"] = Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Churn\"]\n",
    "X = df.drop(columns=[\"Churn\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=VALID_SIZE, random_state=RANDOM_STATE\n",
    ")  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For representing data frames and their relations, featuretools useses entity sets. In our case, we represent the data in a parent-child relation, where the customer dataframe is the parent and billings and subscriptions are the children. By providing our primary keys as an index and having foreign keys present in the parent dataframe, the framework could set up the relations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_wrapper(\n",
    "    id: str,\n",
    "    data_frame_name: str,\n",
    "    feature_dataset: pd.DataFrame,\n",
    "    variable_types: dict,\n",
    "    entity_set_columns: dict,\n",
    ") -> ft.EntitySet:\n",
    "    \"\"\"Non generic wrapper for making ft.EntitySet\"\"\"\n",
    "    es = ft.EntitySet(id=id)\n",
    "\n",
    "    es.add_dataframe(\n",
    "        dataframe_name=data_frame_name,\n",
    "        index=entity_set_columns.get(\"customers\").index,\n",
    "        logical_types=variable_types,\n",
    "        dataframe=feature_dataset,\n",
    "    )\n",
    "\n",
    "    es.normalize_dataframe(\n",
    "        base_dataframe_name=data_frame_name,\n",
    "        new_dataframe_name=entity_set_columns.get(\"billings\").new_dataframe_name,\n",
    "        index=entity_set_columns.get(\"billings\").index,\n",
    "        additional_columns=entity_set_columns.get(\"billings\").additional_columns,\n",
    "    )\n",
    "\n",
    "    es.normalize_dataframe(\n",
    "        base_dataframe_name=data_frame_name,\n",
    "        new_dataframe_name=entity_set_columns.get(\"subscriptions\").new_dataframe_name,\n",
    "        index=entity_set_columns.get(\"subscriptions\").index,\n",
    "        additional_columns=entity_set_columns.get(\"subscriptions\").additional_columns,\n",
    "    )\n",
    "\n",
    "    return es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = entityset_wrapper(\n",
    "    id=\"customers_train\",\n",
    "    data_frame_name=\"customers\",\n",
    "    feature_dataset=X_train,\n",
    "    variable_types=CUSTOMERS | SUBSCRIPTIONS | BILLINGS,\n",
    "    entity_set_columns=entity_set_columns,\n",
    ")\n",
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the entity set in place, we can call the deep feature synthesis function to finally generate the features. For details on how the algorithm works, please refer to the paper of [Kanter and Veeramachaneni: Deep Feature Synthesis:\n",
    "Towards Automating Data Science Endeavors](https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, feature_defs = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_dataframe_name=\"customers\",\n",
    "    max_depth=2,\n",
    "    include_cutoff_time=False,\n",
    ")\n",
    "\n",
    "feature_matrix_enc, features_enc = ft.encode_features(feature_matrix, feature_defs)\n",
    "print(f\"Shape of the resulting feature matrix {feature_matrix_enc.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `max_depth` regulates the depth the generation step takes place, eg depth one only existing features are used as input. Depth two takes the generated features from depth one and replays the algorithm to the results from depth one. \n",
    "\n",
    "The resulting feature encoded matrix contains 118 columns. After feature generation it's common to remove space features and features which do not contain a lot of information, to decrease the risk of overfitting and improve accuracy. In this example, we use two approaches to address the problem, first the build-in functions `remove_highly_null_features` and `remove_single_value_features` from feature tools and also a variance threshold (`VarianceThreshold`) method from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCT_NULL_THRESHOLD = 0.85\n",
    "\n",
    "fm_wo_high_null, f_wo_high_null = remove_highly_null_features(\n",
    "    feature_matrix_enc, features=features_enc, pct_null_threshold=PCT_NULL_THRESHOLD\n",
    ")\n",
    "print(f\"Number of removed features: {len(set(features_enc) - set(f_wo_high_null))}\")\n",
    "\n",
    "fm_wo_sv, f_wo_sv = remove_single_value_features(\n",
    "    fm_wo_high_null, features=f_wo_high_null\n",
    ")\n",
    "print(f\"Number of removed features: {len(set(f_wo_sv) - set(f_wo_high_null))}\")\n",
    "\n",
    "X_train_feature_matrix = fm_wo_sv\n",
    "X_train_features = f_wo_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "numeric_features_ft = get_cols_by_type(\n",
    "    in_df=X_train_feature_matrix,\n",
    "    types_to_search=[np.float64, np.int64],\n",
    "    exclude_ids=False,\n",
    ")\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=NUMERIC_TRANSFORMER_REPLACEMENT)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, numeric_features_ft),\n",
    "    ], verbose=True, remainder = \"passthrough\"\n",
    ")\n",
    "# fmt: on\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE_THRESHOLD = 0.8 * (1 - 0.8)\n",
    "MAX_ITER = 10**4\n",
    "\n",
    "# fmt: off\n",
    "clf = Pipeline(\n",
    "    steps=[ (\"preprocessor\",            preprocessor), \n",
    "            ('feature_selection_var',   VarianceThreshold(threshold=VARIANCE_THRESHOLD)),\n",
    "            (\"classifier\",              xgb.XGBClassifier(objective=\"binary:logistic\", random_state=RANDOM_STATE))],\n",
    "            verbose=True\n",
    "            \n",
    ")\n",
    "# fmt: on\n",
    "\n",
    "clf.fit(X_train_feature_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_val = entityset_wrapper(\n",
    "    id=\"customers_validation\",\n",
    "    data_frame_name=\"customers\",\n",
    "    feature_dataset=X_val,\n",
    "    variable_types=CUSTOMERS | SUBSCRIPTIONS | BILLINGS,\n",
    "    entity_set_columns=entity_set_columns,\n",
    ")\n",
    "\n",
    "val_transformed = ft.calculate_feature_matrix(X_train_features, es_val)\n",
    "\n",
    "\n",
    "es_test = entityset_wrapper(\n",
    "    id=\"customers_test\",\n",
    "    data_frame_name=\"customers\",\n",
    "    feature_dataset=X_test,\n",
    "    variable_types=CUSTOMERS | SUBSCRIPTIONS | BILLINGS,\n",
    "    entity_set_columns=entity_set_columns,\n",
    ")\n",
    "\n",
    "test_transformed = ft.calculate_feature_matrix(X_train_features, es_test)\n",
    "\n",
    "\n",
    "print(\"model score: %.3f\" % clf.score(val_transformed, y_val))\n",
    "print(\"model score: %.3f\" % clf.score(test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_functions = (\n",
    "    ft.encode_features,\n",
    "    remove_highly_null_features,\n",
    "    remove_single_value_features,\n",
    ")\n",
    "\n",
    "\n",
    "# def benchmark_feature_depth(\n",
    "#    feature_matrix_plan: tuple,\n",
    "#    ft_functions: tuple,\n",
    "#    fitting_pipline: Pipeline\n",
    "#    df_tain: tuplepd.DataFrame,\n",
    "#    df_val: pd.DataFrame)\n",
    "#    df_test: tuble\n",
    "# :\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkData:\n",
    "    es: ft.EntitySet\n",
    "    X: pd.DataFrame\n",
    "    y: pd.Series\n",
    "\n",
    "\n",
    "def benchmark(\n",
    "    train_data: BenchmarkData,\n",
    "    test_data: BenchmarkData,\n",
    "    val_data: BenchmarkData,\n",
    "    max_depth: int,\n",
    "    ft_selection_functions: tuple,\n",
    "    fitting_pipe: Pipeline,\n",
    "):\n",
    "\n",
    "    ft_description = ft.dfs(\n",
    "        entityset=train_data.es, target_dataframe_name=\"customers\", max_depth=2\n",
    "    )\n",
    "\n",
    "    ft_selected = reduce(lambda res, f: f(*res), ft_selection_functions, ft_description)\n",
    "\n",
    "    fitting_pipe.fit(ft_selected[0], train_data.y)\n",
    "\n",
    "    val_transformed = ft.calculate_feature_matrix(ft_selected[1], val_data.es)\n",
    "    test_transformed = ft.calculate_feature_matrix(ft_selected[1], test_data.es)\n",
    "\n",
    "    print(\"model score: %.3f\" % clf.score(val_transformed, val_data.y))\n",
    "    print(\"model score: %.3f\" % clf.score(test_transformed, test_data.y))\n",
    "\n",
    "\n",
    "# for fun in ft_functions:\n",
    "# res = ft.encode_features(*feature_matrix_plan)\n",
    "# res = ft.encode_features(*feature_matrix_plan)\n",
    "\n",
    "# from functools import reduce\n",
    "\n",
    "# result = reduce(lambda res, f: f(*res), ft_functions, feature_matrix_plan)\n",
    "# result\n",
    "\n",
    "#    map(ft_functions, encode_features, remove_highly_null_features)#\n",
    "\n",
    "# fm_wo_high_null, f_wo_high_null = remove_highly_null_features(feature_matrix, features=features_enc, pct_null_threshold = .85)\n",
    "# print(f\"Number of removed features: {len(set(features_enc) - set(f_wo_high_null))}\")\n",
    "\n",
    "# fm_wo_sv, f_wo_sv = remove_single_value_features(fm_wo_high_null, features=f_wo_high_null)\n",
    "# print(f\"Number of removed features: {len(set(f_wo_sv) - set(f_wo_high_null))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "result = reduce(lambda res, f: f(res), funcs, val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('data-science-playbooks-Yk-1xU1Q-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "919e6aa0d89df1282d444dc7cd808811badc65a732d2c47612a254902fd6721c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

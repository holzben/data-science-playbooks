{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Pipeline's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a little illustrative example how [scikit-learn's](https://scikit-learn.org/stable/) pipelines can be used for feature engineering and data transformation in one go. The aim of the Notebook is to show some examples how piplines can be used, but it should *not* be considered as a tutorial on Machine Learning, Model selection etc. \n",
    "\n",
    "The main advantage piplines is that for all datasets test, train and validation, the exact same transformations can be done easily, in rather compact manner and without a large amount of overhead code. Also error handling and dealing with code failure can be done on a single place rather then distributed over several classes, functions and interfaces. \n",
    "\n",
    "In this example we gonna apply the following, common steps via via pipelines:\n",
    "- Correcting Missing data via `SimpleImputer`\n",
    "- Scaling the Data using `StandardScaler`\n",
    "- Feature engineering by implementing two custom transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_TO_DATA = (\n",
    "    \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    ")\n",
    "TEST_SIZE = 0.2\n",
    "VALID_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "NUMERIC_TRANSFORMER_REPLACEMENT = \"median\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we are using the classic Titanic survival dataset. For a detailed description how the data looks and what each colum represents please have a look on [Kaggle](https://www.kaggle.com/competitions/titanic/data). As common feature we add the Family Size and correct the Titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of CERTIFICATE_VERIFY_FAILED run Install Certificates.command\n",
    "# see also https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org\n",
    "df = pd.read_csv(filepath_or_buffer=URL_TO_DATA, index_col=0)\n",
    "\n",
    "\n",
    "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "df[\"Title\"] = \"NA\"\n",
    "df[\"Title\"] = df.Name.str.extract(\"([A-Za-z]+)\\.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into test, train and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Survived\"]\n",
    "X = df.drop(columns=[\"Survived\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=VALID_SIZE, random_state=RANDOM_STATE\n",
    ")  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an input for our pipeline we need to classify our columns according to type and which transformations we want to apply to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"Age\", \"Fare\"]\n",
    "categorical_features = [\n",
    "    \"Pclass\",\n",
    "    \"Sex\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Embarked\",\n",
    "    \"Title\",\n",
    "    \"FamilySize\",\n",
    "]\n",
    "discretized_features = [\"FamilySize\"]\n",
    "corrector_features = [\"Title\"]\n",
    "\n",
    "BINS = [0, 1, 2, 4, np.Inf]\n",
    "LABELS = [\"ALONE\", \"SMALL\", \"MED\", \"LARGE\"]\n",
    "\n",
    "\n",
    "CORRECTIONS = {\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Mme\": \"Miss\",\n",
    "    \"Ms\": \"Miss\",\n",
    "    \"Dr\": \"Mr\",\n",
    "    \"Major\": \"Mr\",\n",
    "    \"Lady\": \"Mrs\",\n",
    "    \"Countess\": \"Mrs\",\n",
    "    \"Jonkheer\": \"Other\",\n",
    "    \"Col\": \"Other\",\n",
    "    \"Rev\": \"Other\",\n",
    "    \"Capt\": \"Mr\",\n",
    "    \"Sir\": \"Mr\",\n",
    "    \"Don\": \"Mr\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we take care of the numeric inputs. For filling missing values we can use [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) and provide a strategy how they should be replaced. In this example we use the median, other options provided are mean, most frequent or constant.\n",
    "\n",
    "Scaling is another common practice for training machine learning models since the learning tends to be much harder or might be impossible if features are on completely different scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=NUMERIC_TRANSFORMER_REPLACEMENT)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "numeric_transformer\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleCorrector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Use transformer to correct the Title column and do One Hot Encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        corrections: dict,\n",
    "        encoder: object = None,\n",
    "    ):\n",
    "        self.corrections = corrections\n",
    "        self.encoder = encoder\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y=None) -> object:\n",
    "        return self\n",
    "\n",
    "    def transform(self, X) -> object:\n",
    "        \"\"\"Does the the transform step. In case an encoder is provided, encoding will be done as well.\"\"\"\n",
    "\n",
    "        X_corrected = X.apply(\n",
    "            lambda x: x.replace(self.corrections, self.known_corrections)\n",
    "        )\n",
    "        self.feature_names = X_corrected.columns.tolist()\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            X_corrected = self.encoder.fit_transform(X_corrected)\n",
    "            self.feature_names = self.encoder.get_feature_names_out()\n",
    "\n",
    "        return X_corrected\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None) -> list:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleCorrector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Use transformer to correct the Title column and do One Hot Encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        known_problems: List,\n",
    "        known_corrections: List,\n",
    "        encoder: object = None,\n",
    "    ):\n",
    "        self.known_corrections = known_corrections\n",
    "        self.known_problems = known_problems\n",
    "        self.encoder = encoder\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y=None) -> object:\n",
    "        return self\n",
    "\n",
    "    def transform(self, X) -> object:\n",
    "        \"\"\"Does the the transform step.\n",
    "        In case an encoder is provided, encoding will be done as well.\"\"\"\n",
    "\n",
    "        X_corrected = X.apply(\n",
    "            lambda x: x.replace(list(CORRECTIONS.keys()), list(CORRECTIONS.values()))\n",
    "        )\n",
    "        self.feature_names = X_corrected.columns.tolist()\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            X_corrected = self.encoder.fit_transform(X_corrected)\n",
    "            self.feature_names = self.encoder.get_feature_names_out()\n",
    "\n",
    "        return X_corrected\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None) -> list:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading through XXX on Kaggle he suggested to correct the titles to a smaller set of values. The easiest way would just be using the `replace`function, but we can also do it via transformers. \n",
    "\n",
    "Scikit learn  provides the possibility for writing customized transformers, by simply creating a class and inheritate `BaseEstimator` and `TransformerMixin`. Our class must implement `fit` and `transform` method, but since we are only using the `transform` methode, `fit` will only be a placeholder. \n",
    "\n",
    "The transform method takes the known corrections passed while initializing the class and applies it to all columns provided to the transformer function.\n",
    "\n",
    "Additionally we included two features which should increase useabillity: First the optional direct one hot encodeing and second the `get_feature_names_out` method, which needs do be implemented to get column names after using the custom transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Use transformer to discretize numeric data. Interface to pandas:`~pandas.cut`\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bins: Any, labels: Any = None, encoder: object = None, **kwargs):\n",
    "        self.bins = bins\n",
    "        self.labels = labels\n",
    "        self.kwargs = kwargs\n",
    "        self.encoder = encoder\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_corrected = X.apply(\n",
    "            lambda x: pd.cut(x, bins=self.bins, labels=self.labels, **self.kwargs)\n",
    "        )\n",
    "\n",
    "        self.feature_names = X_corrected.columns.tolist()\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            X_corrected = self.encoder.fit_transform(X_corrected)\n",
    "            self.feature_names = self.encoder.get_feature_names_out()\n",
    "\n",
    "        return X_corrected\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None) -> list:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "one_hot_enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"discretize\", Discretizer(BINS, LABELS, one_hot_enc), discretized_features),\n",
    "        (\"correct\",    TitleCorrector(CORRECTIONS, one_hot_enc), corrector_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"onehot\", one_hot_enc, categorical_features),\n",
    "    ]\n",
    ")\n",
    "preprocessor\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    "\n",
    "df_to_inspect = pd.DataFrame.sparse.from_spmatrix(\n",
    "    preprocess.named_steps[\"preprocessor\"].fit_transform(X_train)\n",
    ")\n",
    "\n",
    "df_to_inspect.columns = preprocess[\"preprocessor\"].get_feature_names_out()\n",
    "# df_to_inspect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "print(\"model score: %.3f\" % clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgement:\n",
    "- Gunes Evitan's Kaggle Notebook on [Titanic - Advanced Feature Engineering Tutorial](https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial/notebook)\n",
    "- Ashwini Swain's Kaggle Notebook SWAIN [EDA To Prediction(DieTanic)](https://www.kaggle.com/ash316/eda-to-prediction-dietanic)\n",
    "- Petro Morales's sklearn Tutorial on [Column Transformer with Mixed Types](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html?highlight=standardscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(preprocess[\"preprocessor\"].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess[\"preprocessor\"].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda x: x.isna().sum(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(CORRECTIONS.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess['preprocessor'].get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17469cfd3d1b513881d871301ae35960f1b42feed1e9d28719ec268723af41bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Pipeline's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a little illustrative example how [scikit-learn's](https://scikit-learn.org/stable/) pipelines can be used for feature engineering and data transformation in one go. The aim of the Notebook is to show some examples how pipelines can be used, but it should *not* be considered as a tutorial on Machine Learning. \n",
    "\n",
    "The one advantage of using pipelines is that for all datasets test, train and validation, the exact same transformations can be done easily, in rather compact manner and without a large amount of overhead code. Also error handling and dealing with code failure can be done on a single place rather then distributed over several classes, functions and interfaces. \n",
    "\n",
    "In this example we gonna apply the following, common steps via via pipelines:\n",
    "- Correcting Missing data via `SimpleImputer`\n",
    "- Scaling the Data using `StandardScaler`\n",
    "- Feature engineering by implementing two custom transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_TO_DATA = (\n",
    "    \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    ")\n",
    "TEST_SIZE = 0.2\n",
    "VALID_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "NUMERIC_TRANSFORMER_REPLACEMENT = \"median\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we are using the classic Titanic survival dataset. For a detailed description how the data looks and what each colum represents please have a look on [Kaggle](https://www.kaggle.com/competitions/titanic/data). As common feature we add the Family Size and correct the Titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of CERTIFICATE_VERIFY_FAILED run Install Certificates.command\n",
    "# see also https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org\n",
    "df = pd.read_csv(filepath_or_buffer=URL_TO_DATA, index_col=0)\n",
    "\n",
    "\n",
    "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "df[\"Title\"] = \"NA\"\n",
    "df[\"Title\"] = df.Name.str.extract(\"([A-Za-z]+)\\.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into test, train and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Survived\"]\n",
    "X = df.drop(columns=[\"Survived\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=VALID_SIZE, random_state=RANDOM_STATE\n",
    ")  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an input for our pipeline we need to classify our columns according to type and which transformations we want to apply to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"Age\", \"Fare\"]\n",
    "categorical_features = [\n",
    "    \"Pclass\",\n",
    "    \"Sex\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Embarked\",\n",
    "    \"Title\",\n",
    "    \"FamilySize\",\n",
    "]\n",
    "cutting_features = [\"FamilySize\"]\n",
    "corrector_features = [\"Title\"]\n",
    "\n",
    "BINS = [0, 1, 2, 4, np.Inf]\n",
    "LABELS = [\"ALONE\", \"SMALL\", \"MED\", \"LARGE\"]\n",
    "\n",
    "\n",
    "CORRECTIONS = {\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Mme\": \"Miss\",\n",
    "    \"Ms\": \"Miss\",\n",
    "    \"Dr\": \"Mr\",\n",
    "    \"Major\": \"Mr\",\n",
    "    \"Lady\": \"Mrs\",\n",
    "    \"Countess\": \"Mrs\",\n",
    "    \"Jonkheer\": \"Other\",\n",
    "    \"Col\": \"Other\",\n",
    "    \"Rev\": \"Other\",\n",
    "    \"Capt\": \"Mr\",\n",
    "    \"Sir\": \"Mr\",\n",
    "    \"Don\": \"Mr\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we take care of the numeric inputs. For filling missing values we can use [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) and provide a strategy how they should be replaced. In this example we use the median, other options provided are mean, most frequent or constant.\n",
    "\n",
    "Scaling is another common practice for training machine learning models since the learning tends to be much harder or might be impossible if features are on completely different scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=NUMERIC_TRANSFORMER_REPLACEMENT)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "numeric_transformer\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categoric variables its even more simple, we just need to add one of the encoders provided in the [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) package and can directly call it as pipeline step, in our case we use a one hot encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "ColumnTransformer(\n",
    "    transformers=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading through Gunes Evitan's advanced feature engineering tutorial on Kaggle he suggested to correct the titles to a smaller set of values. The easiest way would just be using the `replace`function, but we can also do it via transformers. \n",
    "\n",
    "Scikit learn  provides the possibility for writing customized transformers, by simply creating a class and inherit `BaseEstimator` and `TransformerMixin`. Our class must implement `fit` and `transform` method, but since we are only using the `transform` method, `fit` will only be a placeholder. \n",
    "\n",
    "The transform method takes the known corrections passed while initializing the class and applies it to all columns provided to the transformer function.\n",
    "\n",
    "Additionally we included two features which should increase usability: First the optional directly encode by providing one of scikit learns encoders provided in the prepossessing package and second the `get_feature_names_out` method, which needs do be implemented to get column names after using the custom transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleCorrector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Use transformer to correct the Title column and do One Hot Encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        corrections: dict,\n",
    "        encoder: object = None,\n",
    "    ):\n",
    "        self.corrections = corrections\n",
    "        self.encoder = encoder\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y=None) -> object:\n",
    "        return self\n",
    "\n",
    "    def transform(self, X) -> object:\n",
    "        \"\"\"Does the the transform step. In case an encoder is provided, encoding will be done as well.\"\"\"\n",
    "\n",
    "        X_corrected = X.apply(\n",
    "            lambda x: x.replace(list(self.corrections.keys()), list(self.corrections.values()))\n",
    "        )\n",
    "        self.feature_names = X_corrected.columns.tolist()\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            X_corrected = self.encoder.fit_transform(X_corrected)\n",
    "            self.feature_names = self.encoder.get_feature_names_out()\n",
    "\n",
    "        return X_corrected\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None) -> list:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other customized transformer could be for categorizing numerical variables, for example categorize the family size into alone, small medium and big. Again we can write a customized transformer for doing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Use transformer to cut numeric data. Interface to pandas:`~pandas.cut`\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bins: Any, labels: Any = None, encoder: object = None, **kwargs):\n",
    "        self.bins = bins\n",
    "        self.labels = labels\n",
    "        self.kwargs = kwargs\n",
    "        self.encoder = encoder\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_corrected = X.apply(\n",
    "            lambda x: pd.cut(x, bins=self.bins, labels=self.labels, **self.kwargs)\n",
    "        )\n",
    "\n",
    "        self.feature_names = X_corrected.columns.tolist()\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            X_corrected = self.encoder.fit_transform(X_corrected)\n",
    "            self.feature_names = self.encoder.get_feature_names_out()\n",
    "\n",
    "        return X_corrected\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None) -> list:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative a user could simply call \n",
    "`X_test['X_corrected'] = pd.cut(X_test[\"FamilySize\"], bins=BINS, labels=LABELS))` and or in our case, since we are using a closed dataset any how, we could apply the change before splitting. In this case the approach would be valid since the bins are fixed. But for example scaling needs to be done separate on each dataset, otherwise we would spill over information from the training data to the validation data. \n",
    "\n",
    "To see the beauty of of this customize transformer approach we need to imagine a production environment where every day new data flys in and retraining takes place several times. For both use cases we can apply the same code in a compact and easy way.  \n",
    "\n",
    "As a next step we can put all our previous steps together via [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) as one pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "one_hot_enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"discretize\", Cutter(BINS, LABELS, one_hot_enc), cutting_features),\n",
    "        (\"correct\",    TitleCorrector(CORRECTIONS, one_hot_enc), corrector_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"onehot\", one_hot_enc, categorical_features),\n",
    "    ]\n",
    ")\n",
    "preprocessor \n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look how our data will look like after applying all pre precessing steps:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_data = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    "\n",
    "df_to_inspect = pd.DataFrame.sparse.from_spmatrix(\n",
    "    inspect_data.named_steps[\"preprocessor\"].fit_transform(X_train)\n",
    ")\n",
    "\n",
    "df_to_inspect.columns = inspect_data[\"preprocessor\"].get_feature_names_out()\n",
    "df_to_inspect.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_inspect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model is now rather easy, we just add a step in our pipeline and all transformers will be executed before the model gets fitted. In our case it's a simple random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_val, y_val))\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "Pipelines for sure offer a nice and compact way how to execute transformations, standard scaling, and feature engineering for all datasets at once. Especially in a production environment with a complex code base this approach definitely makes sense. Especially if a lot of steps are necessary the approach provides a clear solution and provides already output for nice documentation via the HTML snippets.\n",
    "\n",
    "Nevertheless, it might be overkill for some small use cases with closed datasets or only a few transformation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgement: \n",
    "- Gunes Evitan's Kaggle Notebook on [Titanic - Advanced Feature Engineering Tutorial](https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial/notebook)\n",
    "- Ashwini Swain's Kaggle Notebook SWAIN [EDA To Prediction(DieTanic)](https://www.kaggle.com/ash316/eda-to-prediction-dietanic)\n",
    "- Petro Morales's sklearn Tutorial on [Column Transformer with Mixed Types](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html?highlight=standardscaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('data-science-playbooks-BCpveUor-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bee2308daca9d7493daa3ca6c18986251e2646dc5a84293a86d7cb61b114a88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
